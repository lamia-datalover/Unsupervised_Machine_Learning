{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2403c2d6",
   "metadata": {},
   "source": [
    "### The purpose of this exercise is to use LSA in order to run unsupervised topic extraction on texts and compare the results to the target variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406629ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chekk\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chekk\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chekk\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chekk\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84973589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = vectorizer.get_feature_names_out()\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It’s not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups()\n",
    "print(news.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e48be1",
   "metadata": {},
   "source": [
    "### Storing the object news.data in a DataFrame and call the column text. Then extracting a sample of 5000 rows to begin with after that I added the target variable to this dataframe in order to run analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802e5ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>From: byab314@chpc.utexas.edu (Srinivas Bettad...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>From: kbanaian@bernard.pitzer.claremont.edu (K...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>From: smb@research.att.com (Steven Bellovin)\\n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "3276  From: byab314@chpc.utexas.edu (Srinivas Bettad...      14\n",
       "6162  From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...      13\n",
       "5020  From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...      16\n",
       "7194  From: kbanaian@bernard.pitzer.claremont.edu (K...      18\n",
       "8983  From: smb@research.att.com (Steven Bellovin)\\n...      11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(news.data,columns=['text'])\n",
    "data=data.sample(5000)\n",
    "data['target']=news.target[data.index]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede3bfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>From: byab314@chpc.utexas.edu (Srinivas Bettad...</td>\n",
       "      <td>14</td>\n",
       "      <td>re vandalizing the skyorganization center for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...</td>\n",
       "      <td>13</td>\n",
       "      <td>re is msg sensitivity superstitionnntpposting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>16</td>\n",
       "      <td>re ban all firearms lines 89organization univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>From: kbanaian@bernard.pitzer.claremont.edu (K...</td>\n",
       "      <td>18</td>\n",
       "      <td>re national sales tax the movielines 43organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>From: smb@research.att.com (Steven Bellovin)\\n...</td>\n",
       "      <td>11</td>\n",
       "      <td>clipper chip  technical detailsorganization a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "3276  From: byab314@chpc.utexas.edu (Srinivas Bettad...      14   \n",
       "6162  From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...      13   \n",
       "5020  From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...      16   \n",
       "7194  From: kbanaian@bernard.pitzer.claremont.edu (K...      18   \n",
       "8983  From: smb@research.att.com (Steven Bellovin)\\n...      11   \n",
       "\n",
       "                                             text_clean  \n",
       "3276   re vandalizing the skyorganization center for...  \n",
       "6162   re is msg sensitivity superstitionnntpposting...  \n",
       "5020   re ban all firearms lines 89organization univ...  \n",
       "7194   re national sales tax the movielines 43organi...  \n",
       "8983   clipper chip  technical detailsorganization a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This line of code applies a lambda function to each \n",
    "value in the 'text' column. The lambda function splits the text at the string \"Subject:\" \n",
    "and retains the portion after \"Subject:\". This effectively removes any content that appears before \"Subject:\" in each text.\n",
    "'''\n",
    "data['text_clean'] = data['text'].apply(lambda x: x.split(\"Subject:\")[1])\n",
    "'''\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: ''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "This line of code applies another lambda function to each value in the 'text_clean' column. The lambda function \n",
    "iterates through each character in the text and keeps only alphanumeric characters \n",
    "(letters and numbers) and spaces. It removes any other characters, effectively cleaning the text.\n",
    "'''\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: ''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "'''\n",
    "This line of code applies a third lambda function to the \n",
    "'text_clean' column. It converts all the text to lowercase using the .lower() method and fills any missing values \n",
    "(NaNs) with an empty string ('') to avoid potential errors during the lowercase conversion.\n",
    "'''\n",
    "data['text_clean'] = data['text_clean'].fillna('').apply(lambda x: x.lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77233e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e342ae",
   "metadata": {},
   "source": [
    "### Tokenizing the cleaned sentences and removing english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e00e706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>From: byab314@chpc.utexas.edu (Srinivas Bettad...</td>\n",
       "      <td>14</td>\n",
       "      <td>re vandalizing the skyorganization center for...</td>\n",
       "      <td>[ , vandalize, skyorganization, center, space,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...</td>\n",
       "      <td>13</td>\n",
       "      <td>re is msg sensitivity superstitionnntpposting...</td>\n",
       "      <td>[ , msg, sensitivity, superstitionnntppostingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>16</td>\n",
       "      <td>re ban all firearms lines 89organization univ...</td>\n",
       "      <td>[ , ban, firearm, line, 89organization, univer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>From: kbanaian@bernard.pitzer.claremont.edu (K...</td>\n",
       "      <td>18</td>\n",
       "      <td>re national sales tax the movielines 43organi...</td>\n",
       "      <td>[ , national, sale, tax, movieline, 43organiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>From: smb@research.att.com (Steven Bellovin)\\n...</td>\n",
       "      <td>11</td>\n",
       "      <td>clipper chip  technical detailsorganization a...</td>\n",
       "      <td>[ , clipper, chip,  , technical, detailsorgani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "3276  From: byab314@chpc.utexas.edu (Srinivas Bettad...      14   \n",
       "6162  From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...      13   \n",
       "5020  From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...      16   \n",
       "7194  From: kbanaian@bernard.pitzer.claremont.edu (K...      18   \n",
       "8983  From: smb@research.att.com (Steven Bellovin)\\n...      11   \n",
       "\n",
       "                                             text_clean  \\\n",
       "3276   re vandalizing the skyorganization center for...   \n",
       "6162   re is msg sensitivity superstitionnntpposting...   \n",
       "5020   re ban all firearms lines 89organization univ...   \n",
       "7194   re national sales tax the movielines 43organi...   \n",
       "8983   clipper chip  technical detailsorganization a...   \n",
       "\n",
       "                                         text_tokenized  \n",
       "3276  [ , vandalize, skyorganization, center, space,...  \n",
       "6162  [ , msg, sensitivity, superstitionnntppostingh...  \n",
       "5020  [ , ban, firearm, line, 89organization, univer...  \n",
       "7194  [ , national, sale, tax, movieline, 43organiza...  \n",
       "8983  [ , clipper, chip,  , technical, detailsorgani...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "data[\"text_tokenized\"] = data[\"text_clean\"].apply(lambda x: [token.lemma_ for token in nlp(x) if token.text not in STOP_WORDS])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4283115",
   "metadata": {},
   "source": [
    "### I detokenized the text because  detokenization reconstructs the original text, preserving the contextual information between words. TF-IDF relies on the relationships between words within a document, and detokenization helps maintain these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cab065c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>nlp_ready</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>From: byab314@chpc.utexas.edu (Srinivas Bettad...</td>\n",
       "      <td>14</td>\n",
       "      <td>re vandalizing the skyorganization center for...</td>\n",
       "      <td>[ , vandalize, skyorganization, center, space,...</td>\n",
       "      <td>vandalize skyorganization center space resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...</td>\n",
       "      <td>13</td>\n",
       "      <td>re is msg sensitivity superstitionnntpposting...</td>\n",
       "      <td>[ , msg, sensitivity, superstitionnntppostingh...</td>\n",
       "      <td>msg sensitivity superstitionnntppostinghost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>16</td>\n",
       "      <td>re ban all firearms lines 89organization univ...</td>\n",
       "      <td>[ , ban, firearm, line, 89organization, univer...</td>\n",
       "      <td>ban firearm line 89organization university t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>From: kbanaian@bernard.pitzer.claremont.edu (K...</td>\n",
       "      <td>18</td>\n",
       "      <td>re national sales tax the movielines 43organi...</td>\n",
       "      <td>[ , national, sale, tax, movieline, 43organiza...</td>\n",
       "      <td>national sale tax movieline 43organization p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>From: smb@research.att.com (Steven Bellovin)\\n...</td>\n",
       "      <td>11</td>\n",
       "      <td>clipper chip  technical detailsorganization a...</td>\n",
       "      <td>[ , clipper, chip,  , technical, detailsorgani...</td>\n",
       "      <td>clipper chip   technical detailsorganization...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "3276  From: byab314@chpc.utexas.edu (Srinivas Bettad...      14   \n",
       "6162  From: julie@eddie.jpl.nasa.gov (Julie Kangas)\\...      13   \n",
       "5020  From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...      16   \n",
       "7194  From: kbanaian@bernard.pitzer.claremont.edu (K...      18   \n",
       "8983  From: smb@research.att.com (Steven Bellovin)\\n...      11   \n",
       "\n",
       "                                             text_clean  \\\n",
       "3276   re vandalizing the skyorganization center for...   \n",
       "6162   re is msg sensitivity superstitionnntpposting...   \n",
       "5020   re ban all firearms lines 89organization univ...   \n",
       "7194   re national sales tax the movielines 43organi...   \n",
       "8983   clipper chip  technical detailsorganization a...   \n",
       "\n",
       "                                         text_tokenized  \\\n",
       "3276  [ , vandalize, skyorganization, center, space,...   \n",
       "6162  [ , msg, sensitivity, superstitionnntppostingh...   \n",
       "5020  [ , ban, firearm, line, 89organization, univer...   \n",
       "7194  [ , national, sale, tax, movieline, 43organiza...   \n",
       "8983  [ , clipper, chip,  , technical, detailsorgani...   \n",
       "\n",
       "                                              nlp_ready  \n",
       "3276    vandalize skyorganization center space resea...  \n",
       "6162    msg sensitivity superstitionnntppostinghost ...  \n",
       "5020    ban firearm line 89organization university t...  \n",
       "7194    national sale tax movieline 43organization p...  \n",
       "8983    clipper chip   technical detailsorganization...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_doc = []\n",
    "for sentence in data[\"text_tokenized\"]:\n",
    "    t = ' '.join(sentence)\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "data['nlp_ready'] = detokenized_doc\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44aaf2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x120528 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 465259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF vector\n",
    "vectorizer = TfidfVectorizer(stop_words='english', smooth_idf=True)\n",
    "X = vectorizer.fit_transform(data['nlp_ready'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744ccb1",
   "metadata": {},
   "source": [
    "### Using the truncatedSVD model in order to create a topic model with 20 different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43da04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>0.074965</td>\n",
       "      <td>-0.022421</td>\n",
       "      <td>-0.004137</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>-0.011081</td>\n",
       "      <td>-0.034196</td>\n",
       "      <td>-0.059725</td>\n",
       "      <td>0.055858</td>\n",
       "      <td>-0.008884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017380</td>\n",
       "      <td>-0.004989</td>\n",
       "      <td>-0.013664</td>\n",
       "      <td>0.073894</td>\n",
       "      <td>0.028447</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>0.028998</td>\n",
       "      <td>-0.028950</td>\n",
       "      <td>-0.010301</td>\n",
       "      <td>vandalize skyorganization center space resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>0.092245</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>-0.010487</td>\n",
       "      <td>-0.006485</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>-0.076698</td>\n",
       "      <td>-0.077743</td>\n",
       "      <td>-0.021321</td>\n",
       "      <td>-0.160918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300075</td>\n",
       "      <td>-0.228565</td>\n",
       "      <td>-0.170249</td>\n",
       "      <td>-0.031156</td>\n",
       "      <td>-0.155546</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>-0.057353</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>msg sensitivity superstitionnntppostinghost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>0.150243</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>-0.060755</td>\n",
       "      <td>0.020706</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>-0.078434</td>\n",
       "      <td>0.023562</td>\n",
       "      <td>-0.037431</td>\n",
       "      <td>-0.026994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058019</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>-0.036074</td>\n",
       "      <td>-0.023726</td>\n",
       "      <td>-0.032649</td>\n",
       "      <td>0.029115</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>ban firearm line 89organization university t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>0.105857</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>-0.028366</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.026506</td>\n",
       "      <td>-0.046810</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>-0.009283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029962</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>-0.008509</td>\n",
       "      <td>-0.001378</td>\n",
       "      <td>0.057719</td>\n",
       "      <td>0.064402</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>national sale tax movieline 43organization p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>0.062543</td>\n",
       "      <td>-0.030219</td>\n",
       "      <td>-0.017358</td>\n",
       "      <td>0.091237</td>\n",
       "      <td>-0.067130</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.061859</td>\n",
       "      <td>-0.036516</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>0.026656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.019219</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>-0.014366</td>\n",
       "      <td>-0.002965</td>\n",
       "      <td>-0.009798</td>\n",
       "      <td>0.022638</td>\n",
       "      <td>clipper chip   technical detailsorganization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>0.116433</td>\n",
       "      <td>-0.058788</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.046273</td>\n",
       "      <td>-0.019753</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>-0.011300</td>\n",
       "      <td>-0.029359</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>-0.008878</td>\n",
       "      <td>0.035777</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.018710</td>\n",
       "      <td>-0.036709</td>\n",
       "      <td>-0.084170</td>\n",
       "      <td>0.016645</td>\n",
       "      <td>desktop rebuild datadesk keyboardorganizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>0.173040</td>\n",
       "      <td>0.039295</td>\n",
       "      <td>-0.036561</td>\n",
       "      <td>0.020575</td>\n",
       "      <td>0.011127</td>\n",
       "      <td>-0.002130</td>\n",
       "      <td>-0.038132</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>-0.018560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>-0.026104</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>-0.020551</td>\n",
       "      <td>0.076115</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>-0.034375</td>\n",
       "      <td>clinton press briefing george stephanopoulos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>0.058832</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.004280</td>\n",
       "      <td>-0.009957</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>-0.013922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010211</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>-0.008591</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>-0.005448</td>\n",
       "      <td>0.021428</td>\n",
       "      <td>0.015635</td>\n",
       "      <td>political atheistsorganization california in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>0.082544</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>-0.007697</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>-0.055889</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007576</td>\n",
       "      <td>0.028126</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>-0.005173</td>\n",
       "      <td>-0.008753</td>\n",
       "      <td>-0.004893</td>\n",
       "      <td>0.026299</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>0.082808</td>\n",
       "      <td>rgv posingnntppostinghost 13320625121replyto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7179</th>\n",
       "      <td>0.116263</td>\n",
       "      <td>0.030384</td>\n",
       "      <td>0.027803</td>\n",
       "      <td>-0.016670</td>\n",
       "      <td>-0.011941</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>-0.017549</td>\n",
       "      <td>-0.051685</td>\n",
       "      <td>0.029460</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.015313</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>0.059158</td>\n",
       "      <td>0.038097</td>\n",
       "      <td>-0.051492</td>\n",
       "      <td>0.086324</td>\n",
       "      <td>-0.070718</td>\n",
       "      <td>-0.031881</td>\n",
       "      <td>pantheism   environmentalismorganization ai ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_1   topic_2   topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "3276  0.074965 -0.022421 -0.004137 -0.008018  0.001517 -0.011081 -0.034196   \n",
       "6162  0.092245  0.023024 -0.010487 -0.006485  0.005923  0.006427 -0.076698   \n",
       "5020  0.150243  0.012916 -0.060755  0.020706  0.005235  0.033383 -0.078434   \n",
       "7194  0.105857  0.001634 -0.028366  0.006724  0.002557  0.026506 -0.046810   \n",
       "8983  0.062543 -0.030219 -0.017358  0.091237 -0.067130  0.015193  0.061859   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3151  0.116433 -0.058788  0.008606  0.046273 -0.019753  0.005657  0.032538   \n",
       "9659  0.173040  0.039295 -0.036561  0.020575  0.011127 -0.002130 -0.038132   \n",
       "1177  0.058832  0.010135  0.009480  0.009608 -0.000166 -0.004280 -0.009957   \n",
       "5410  0.082544  0.017567 -0.020974  0.017759 -0.007697  0.018213 -0.055889   \n",
       "7179  0.116263  0.030384  0.027803 -0.016670 -0.011941 -0.020597 -0.017549   \n",
       "\n",
       "       topic_8   topic_9  topic_10  ...  topic_12  topic_13  topic_14  \\\n",
       "3276 -0.059725  0.055858 -0.008884  ... -0.017380 -0.004989 -0.013664   \n",
       "6162 -0.077743 -0.021321 -0.160918  ...  0.300075 -0.228565 -0.170249   \n",
       "5020  0.023562 -0.037431 -0.026994  ... -0.058019  0.004025  0.009290   \n",
       "7194  0.000637  0.011300 -0.009283  ... -0.029962  0.000841  0.024208   \n",
       "8983 -0.036516  0.011107  0.026656  ...  0.004396  0.003004  0.001508   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3151 -0.011300 -0.029359  0.005873  ...  0.049022 -0.008878  0.035777   \n",
       "9659  0.001673  0.007711 -0.018560  ...  0.002722 -0.012818  0.034048   \n",
       "1177  0.003347  0.000532 -0.013922  ... -0.010211 -0.001274 -0.008591   \n",
       "5410  0.015982 -0.003849  0.021684  ... -0.007576  0.028126 -0.001709   \n",
       "7179 -0.051685  0.029460  0.008214  ...  0.005715  0.015313 -0.041914   \n",
       "\n",
       "      topic_15  topic_16  topic_17  topic_18  topic_19  topic_20  \\\n",
       "3276  0.073894  0.028447  0.024238  0.028998 -0.028950 -0.010301   \n",
       "6162 -0.031156 -0.155546  0.020133  0.012763 -0.057353  0.017282   \n",
       "5020  0.022828 -0.036074 -0.023726 -0.032649  0.029115  0.004773   \n",
       "7194  0.000455 -0.008509 -0.001378  0.057719  0.064402  0.002184   \n",
       "8983  0.019219  0.001643 -0.014366 -0.002965 -0.009798  0.022638   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "3151 -0.000636 -0.000401 -0.018710 -0.036709 -0.084170  0.016645   \n",
       "9659 -0.026104  0.051384 -0.020551  0.076115  0.016873 -0.034375   \n",
       "1177  0.001183  0.014641 -0.002215 -0.005448  0.021428  0.015635   \n",
       "5410 -0.005173 -0.008753 -0.004893  0.026299  0.033752  0.082808   \n",
       "7179  0.059158  0.038097 -0.051492  0.086324 -0.070718 -0.031881   \n",
       "\n",
       "                                                   text  \n",
       "3276    vandalize skyorganization center space resea...  \n",
       "6162    msg sensitivity superstitionnntppostinghost ...  \n",
       "5020    ban firearm line 89organization university t...  \n",
       "7194    national sale tax movieline 43organization p...  \n",
       "8983    clipper chip   technical detailsorganization...  \n",
       "...                                                 ...  \n",
       "3151    desktop rebuild datadesk keyboardorganizatio...  \n",
       "9659    clinton press briefing george stephanopoulos...  \n",
       "1177    political atheistsorganization california in...  \n",
       "5410    rgv posingnntppostinghost 13320625121replyto...  \n",
       "7179    pantheism   environmentalismorganization ai ...  \n",
       "\n",
       "[5000 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100)\n",
    "lsa = svd_model.fit_transform(X)\n",
    "\n",
    "topic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_{}\".format(i+1) for i in range(20)], index = data.index)\n",
    "topic_encoded_df[\"text\"] = data['nlp_ready'].values\n",
    "topic_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cdea602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3810\n",
       "9      138\n",
       "3      134\n",
       "5      112\n",
       "1      103\n",
       "8      100\n",
       "19      75\n",
       "16      74\n",
       "12      72\n",
       "4       62\n",
       "14      57\n",
       "6       48\n",
       "7       39\n",
       "15      32\n",
       "13      31\n",
       "10      30\n",
       "18      28\n",
       "2       21\n",
       "11      17\n",
       "17      17\n",
       "Name: class_pred, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using np.argmax to have the index of the maximal element\n",
    "topic_encoded_df[\"class_pred\"] = [np.argmax(topic) for topic in lsa]\n",
    "topic_encoded_df[\"class_pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea7f8b",
   "metadata": {},
   "source": [
    "### Add the target variable to the topic model dataframe and print the confusion matrix for the topic against the target variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94102075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    284\n",
       "14    279\n",
       "15    278\n",
       "6     265\n",
       "2     265\n",
       "4     265\n",
       "3     262\n",
       "11    260\n",
       "17    259\n",
       "10    259\n",
       "1     256\n",
       "9     254\n",
       "5     251\n",
       "8     250\n",
       "7     246\n",
       "16    240\n",
       "12    236\n",
       "18    223\n",
       "0     204\n",
       "19    164\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_encoded_df[\"target\"] = news.target[data.index]\n",
    "topic_encoded_df.head()\n",
    "topic_encoded_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7903b14",
   "metadata": {},
   "source": [
    "### We conclude that LSA is based on the hypothesis that a given document can be related to several topics. This makes the interpretation of the model's output more complicated, but allows to create topic models that are more realistic (because in real life, a document is often related to different topics !)\n",
    "### Note that LSA is very convenient to find some structure among a text corpus, but it usually creates topics that are quite different from the categories that would have been determined by a human,  this is why the topics found by LSA are very different from the target !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ab8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cb0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfe8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c7c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b9b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ab0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565efe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca365efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80525959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66d79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77044f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefa55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1a02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c89a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe67fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
